{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "614f6ac9",
   "metadata": {},
   "source": [
    "# Anomaly Detection in Cognitive Trajectories: Identifying Atypical Alzheimer's Progression\n",
    "\n",
    "This notebook demonstrates a comprehensive analysis of cognitive decline patterns using unsupervised anomaly detection methods to identify atypical Alzheimer's disease progression.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction and Methodology](#introduction)\n",
    "2. [Data Generation and Exploration](#data-generation)\n",
    "3. [Feature Engineering](#feature-engineering)\n",
    "4. [Anomaly Detection](#anomaly-detection)\n",
    "5. [Results Visualization](#visualization)\n",
    "6. [Clinical Interpretation](#interpretation)\n",
    "7. [Cross-Validation](#cross-validation)\n",
    "8. [Conclusions](#conclusions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a421df49",
   "metadata": {},
   "source": [
    "## 1. Introduction and Methodology {#introduction}\n",
    "\n",
    "### Background\n",
    "Alzheimer's disease typically follows predictable patterns of cognitive decline, but some patients exhibit atypical progression patterns that may indicate:\n",
    "- Rapid cognitive decline\n",
    "- Unusual plateau phases\n",
    "- Fluctuating cognitive performance\n",
    "- Resistance to typical decline patterns\n",
    "\n",
    "### Methodology\n",
    "We use two complementary unsupervised machine learning approaches:\n",
    "\n",
    "**Isolation Forest**: Tree-based algorithm that isolates anomalies by randomly selecting features and split values. Anomalies are isolated closer to the root of the tree.\n",
    "\n",
    "**DBSCAN**: Density-based clustering that identifies core points, border points, and noise points. Noise points represent potential anomalies.\n",
    "\n",
    "### Cognitive Assessments\n",
    "- **MMSE** (Mini-Mental State Examination): 0-30 scale, higher scores indicate better cognition\n",
    "- **FAQ** (Functional Activities Questionnaire): 0-30 scale, higher scores indicate more impairment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143b825d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "# Import project modules\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "\n",
    "from data_loader import RealDatasetLoader\n",
    "from preprocessing import TrajectoryFeatureEngine, DataValidator\n",
    "from anomaly_detection import AnomalyDetector\n",
    "from visualization import AnomalyVisualizer\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361b4c27",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Exploration {#data-loading}\n",
    "\n",
    "We load and harmonize the longitudinal ADNI dataset to explore cognitive trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163f90bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load harmonized real dataset\n",
    "loader = RealDatasetLoader('../config/config.yaml')\n",
    "df = loader.load_dataset()\n",
    "summary = loader.get_summary()\n",
    "\n",
    "print(f\"Dataset loaded with {df['patient_id'].nunique()} patients and {len(df)} visits\")\n",
    "print(\"\\nKey summary statistics:\")\n",
    "for key, value in summary.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Display first few rows\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4ebef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality assessment\n",
    "validator = DataValidator()\n",
    "visit_months = [0, 6, 12, 18, 24]\n",
    "quality_metrics = validator.validate_data_quality(df, visit_months)\n",
    "validator.print_quality_report(quality_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b034ff97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize data distribution\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Dataset Overview', fontsize=16, fontweight='bold')\n",
    "\n",
    "# MMSE distribution by visit\n",
    "sns.boxplot(data=df, x='visit_month', y='mmse', ax=axes[0, 0])\n",
    "axes[0, 0].set_title('MMSE Distribution by Visit')\n",
    "axes[0, 0].set_xlabel('Visit Month')\n",
    "axes[0, 0].set_ylabel('MMSE Score')\n",
    "\n",
    "# FAQ distribution by visit\n",
    "sns.boxplot(data=df, x='visit_month', y='faq', ax=axes[0, 1])\n",
    "axes[0, 1].set_title('FAQ Distribution by Visit')\n",
    "axes[0, 1].set_xlabel('Visit Month')\n",
    "axes[0, 1].set_ylabel('FAQ Score')\n",
    "\n",
    "# Pattern type distribution\n",
    "pattern_counts = df['pattern_type'].value_counts()\n",
    "axes[0, 2].pie(pattern_counts.values, labels=pattern_counts.index, autopct='%1.1f%%')\n",
    "axes[0, 2].set_title('Pattern Type Distribution')\n",
    "\n",
    "# Demographics\n",
    "demo_df = df.groupby('patient_id').first()\n",
    "sns.histplot(data=demo_df, x='age', bins=20, ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Age Distribution')\n",
    "axes[1, 0].set_xlabel('Age')\n",
    "\n",
    "sns.countplot(data=demo_df, x='apoe4_copies', ax=axes[1, 1])\n",
    "axes[1, 1].set_title('APOE4 Copies Distribution')\n",
    "axes[1, 1].set_xlabel('APOE4 Copies')\n",
    "\n",
    "# Missing data heatmap\n",
    "missing_data = df.pivot_table(index='patient_id', columns='visit_month', values='mmse', aggfunc='count')\n",
    "missing_data = missing_data.isnull().astype(int)\n",
    "sns.heatmap(missing_data.head(50), cmap='Reds', cbar=True, ax=axes[1, 2])\n",
    "axes[1, 2].set_title('Missing Data Pattern (First 50 Patients)')\n",
    "axes[1, 2].set_xlabel('Visit Month')\n",
    "axes[1, 2].set_ylabel('Patient ID')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118448cb",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering {#feature-engineering}\n",
    "\n",
    "We transform longitudinal trajectories into feature vectors capturing:\n",
    "- Raw time series values\n",
    "- Trajectory slopes and acceleration\n",
    "- Variability measures\n",
    "- Change from baseline\n",
    "- Cross-measure ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f06516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering\n",
    "feature_engine = TrajectoryFeatureEngine('../config/config.yaml')\n",
    "patient_ids, raw_features, processed_features = feature_engine.build_trajectory_features(df)\n",
    "\n",
    "print(f\"Feature engineering results:\")\n",
    "print(f\"  Valid patients: {len(patient_ids)}\")\n",
    "print(f\"  Raw features shape: {raw_features.shape}\")\n",
    "print(f\"  Processed features shape: {processed_features.shape}\")\n",
    "print(f\"  PCA explained variance: {feature_engine.pca.explained_variance_ratio_.sum():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb9acb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Feature Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Raw feature correlation\n",
    "feature_sample = raw_features[:, :20]  # First 20 features for visualization\n",
    "corr_matrix = np.corrcoef(feature_sample.T)\n",
    "sns.heatmap(corr_matrix, cmap='coolwarm', center=0, ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Feature Correlation Matrix (Sample)')\n",
    "\n",
    "# PCA component importance\n",
    "pca_importance = feature_engine.pca.explained_variance_ratio_[:10]\n",
    "axes[0, 1].bar(range(len(pca_importance)), pca_importance)\n",
    "axes[0, 1].set_title('PCA Component Importance (Top 10)')\n",
    "axes[0, 1].set_xlabel('Principal Component')\n",
    "axes[0, 1].set_ylabel('Explained Variance Ratio')\n",
    "\n",
    "# Feature distribution (first two PCs)\n",
    "axes[1, 0].scatter(processed_features[:, 0], processed_features[:, 1], alpha=0.6)\n",
    "axes[1, 0].set_title('Feature Space (PC1 vs PC2)')\n",
    "axes[1, 0].set_xlabel('First Principal Component')\n",
    "axes[1, 0].set_ylabel('Second Principal Component')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Cumulative explained variance\n",
    "cumvar = np.cumsum(feature_engine.pca.explained_variance_ratio_)\n",
    "axes[1, 1].plot(range(1, len(cumvar)+1), cumvar, 'bo-')\n",
    "axes[1, 1].axhline(y=0.9, color='r', linestyle='--', label='90% Variance')\n",
    "axes[1, 1].set_title('Cumulative Explained Variance')\n",
    "axes[1, 1].set_xlabel('Number of Components')\n",
    "axes[1, 1].set_ylabel('Cumulative Explained Variance')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e17d403",
   "metadata": {},
   "source": [
    "## 4. Anomaly Detection {#anomaly-detection}\n",
    "\n",
    "We apply two complementary anomaly detection methods and analyze their consensus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667733be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize anomaly detector\n",
    "detector = AnomalyDetector('../config/config.yaml')\n",
    "\n",
    "# Fit Isolation Forest\n",
    "detector.fit_isolation_forest(processed_features)\n",
    "iso_predictions, iso_scores = detector.predict_isolation_forest(processed_features)\n",
    "\n",
    "print(f\"Isolation Forest Results:\")\n",
    "print(f\"  Anomalies detected: {np.sum(iso_predictions == -1)} ({np.mean(iso_predictions == -1):.1%})\")\n",
    "print(f\"  Score range: {iso_scores.min():.3f} to {iso_scores.max():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3f5b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit DBSCAN\n",
    "detector.fit_dbscan(processed_features)\n",
    "dbscan_predictions, cluster_labels = detector.predict_dbscan(processed_features)\n",
    "\n",
    "n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
    "n_noise = list(cluster_labels).count(-1)\n",
    "\n",
    "print(f\"DBSCAN Results:\")\n",
    "print(f\"  Number of clusters: {n_clusters}\")\n",
    "print(f\"  Noise points: {n_noise} ({n_noise/len(cluster_labels):.1%})\")\n",
    "print(f\"  Optimal eps: {detector.optimal_eps:.3f}\")\n",
    "print(f\"  Optimal min_samples: {detector.optimal_min_samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137511aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consensus analysis\n",
    "consensus = detector.consensus_anomaly_detection(iso_predictions, dbscan_predictions)\n",
    "\n",
    "print(f\"Consensus Analysis:\")\n",
    "print(f\"  Isolation Forest only: {consensus['iso_count']}\")\n",
    "print(f\"  DBSCAN only: {consensus['dbscan_count']}\")\n",
    "print(f\"  Both methods (intersection): {consensus['intersection_count']}\")\n",
    "print(f\"  Either method (union): {consensus['union_count']}\")\n",
    "print(f\"  Agreement rate: {consensus['agreement_rate']:.3f}\")\n",
    "\n",
    "# Rank anomalies\n",
    "anomaly_ranks, combined_scores = detector.rank_anomalies(iso_scores, dbscan_predictions)\n",
    "\n",
    "print(f\"\\nTop 10 Most Anomalous Patients:\")\n",
    "for i in range(10):\n",
    "    idx = anomaly_ranks[i]\n",
    "    pid = patient_ids[idx]\n",
    "    score = combined_scores[idx]\n",
    "    iso_anom = iso_predictions[idx] == -1\n",
    "    db_anom = dbscan_predictions[idx]\n",
    "    print(f\"  {i+1:2d}. Patient {pid}: {score:.3f} (ISO: {iso_anom}, DB: {db_anom})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7129059f",
   "metadata": {},
   "source": [
    "## 5. Results Visualization {#visualization}\n",
    "\n",
    "We create comprehensive visualizations to understand the anomaly detection results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89854e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize visualizer\n",
    "visualizer = AnomalyVisualizer('../config/config.yaml')\n",
    "\n",
    "# Prepare anomaly results for visualization\n",
    "anomaly_results = {\n",
    "    'iso_predictions': iso_predictions,\n",
    "    'iso_scores': iso_scores,\n",
    "    'dbscan_predictions': dbscan_predictions,\n",
    "    'cluster_labels': cluster_labels,\n",
    "    'consensus': consensus,\n",
    "    'anomaly_ranks': anomaly_ranks,\n",
    "    'combined_scores': combined_scores\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704d12e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trajectory overview\n",
    "fig = visualizer.plot_trajectory_overview(df, patient_ids, consensus)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fe6e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anomaly embedding visualization\n",
    "fig = visualizer.plot_anomaly_embedding(processed_features, patient_ids, anomaly_results)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918da428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster analysis\n",
    "fig = visualizer.plot_cluster_analysis(processed_features, cluster_labels, patient_ids)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64ecb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed analysis of top anomalies\n",
    "top_anomaly_patients = patient_ids[anomaly_ranks[:10]]\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
    "fig.suptitle('Top 10 Most Anomalous Patient Trajectories', fontsize=16, fontweight='bold')\n",
    "\n",
    "for i, pid in enumerate(top_anomaly_patients):\n",
    "    row = i // 5\n",
    "    col = i % 5\n",
    "    \n",
    "    patient_data = df[df['patient_id'] == pid]\n",
    "    \n",
    "    # Plot MMSE and FAQ trajectories\n",
    "    ax = axes[row, col]\n",
    "    ax2 = ax.twinx()\n",
    "    \n",
    "    line1 = ax.plot(patient_data['visit_month'], patient_data['mmse'], \n",
    "                   'b-o', linewidth=2, markersize=6, label='MMSE')\n",
    "    line2 = ax2.plot(patient_data['visit_month'], patient_data['faq'], \n",
    "                    'r-s', linewidth=2, markersize=6, label='FAQ')\n",
    "    \n",
    "    ax.set_xlabel('Visit Month')\n",
    "    ax.set_ylabel('MMSE', color='blue')\n",
    "    ax2.set_ylabel('FAQ', color='red')\n",
    "    \n",
    "    # Get pattern type and anomaly score\n",
    "    pattern = patient_data['pattern_type'].iloc[0]\n",
    "    score = combined_scores[patient_ids == pid][0]\n",
    "    \n",
    "    ax.set_title(f'Patient {pid}\\n{pattern} (Score: {score:.3f})', fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Color coding by pattern type\n",
    "    if pattern == 'rapid_decline':\n",
    "        ax.patch.set_facecolor('#ffeeee')\n",
    "    elif pattern == 'plateau':\n",
    "        ax.patch.set_facecolor('#eeffee')\n",
    "    elif pattern == 'fluctuating':\n",
    "        ax.patch.set_facecolor('#eeeeff')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a896a2b2",
   "metadata": {},
   "source": [
    "## 6. Clinical Interpretation {#interpretation}\n",
    "\n",
    "We analyze the clinical characteristics of detected anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45f9886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create patient-level results dataframe\n",
    "patient_results = pd.DataFrame({\n",
    "    'patient_id': patient_ids,\n",
    "    'iso_score': iso_scores,\n",
    "    'iso_anomaly': iso_predictions == -1,\n",
    "    'dbscan_noise': dbscan_predictions,\n",
    "    'cluster_label': cluster_labels,\n",
    "    'combined_score': combined_scores\n",
    "})\n",
    "\n",
    "# Add demographics and clinical variables\n",
    "demo_data = df.groupby('patient_id').first()[\n",
    "    ['age', 'gender', 'education', 'apoe4_copies', 'pattern_type']\n",
    "].reset_index()\n",
    "\n",
    "patient_results = patient_results.merge(demo_data, on='patient_id', how='left')\n",
    "\n",
    "# Analyze anomaly characteristics\n",
    "print(\"Clinical Characteristics Analysis\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# By pattern type\n",
    "print(\"\\nAnomalies by True Pattern Type:\")\n",
    "anomaly_by_pattern = patient_results.groupby('pattern_type')[\n",
    "    ['iso_anomaly', 'dbscan_noise']\n",
    "].agg(['sum', 'count', 'mean'])\n",
    "print(anomaly_by_pattern)\n",
    "\n",
    "# By APOE4 status\n",
    "print(\"\\nAnomalies by APOE4 Status:\")\n",
    "anomaly_by_apoe = patient_results.groupby('apoe4_copies')[\n",
    "    ['iso_anomaly', 'dbscan_noise']\n",
    "].agg(['sum', 'count', 'mean'])\n",
    "print(anomaly_by_apoe)\n",
    "\n",
    "# Age analysis\n",
    "print(\"\\nAge Analysis:\")\n",
    "normal_age = patient_results[~patient_results['iso_anomaly']]['age'].mean()\n",
    "anomaly_age = patient_results[patient_results['iso_anomaly']]['age'].mean()\n",
    "print(f\"Normal patients mean age: {normal_age:.1f}\")\n",
    "print(f\"Anomalous patients mean age: {anomaly_age:.1f}\")\n",
    "print(f\"Age difference: {anomaly_age - normal_age:.1f} years\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e439ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of clinical characteristics\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Clinical Characteristics of Anomalies', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Pattern type distribution\n",
    "pattern_counts = patient_results.groupby('pattern_type')['iso_anomaly'].agg(['sum', 'count'])\n",
    "pattern_rates = pattern_counts['sum'] / pattern_counts['count']\n",
    "pattern_rates.plot(kind='bar', ax=axes[0, 0], color='skyblue')\n",
    "axes[0, 0].set_title('Anomaly Rate by Pattern Type')\n",
    "axes[0, 0].set_ylabel('Anomaly Rate')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# APOE4 distribution\n",
    "apoe_counts = patient_results.groupby('apoe4_copies')['iso_anomaly'].agg(['sum', 'count'])\n",
    "apoe_rates = apoe_counts['sum'] / apoe_counts['count']\n",
    "apoe_rates.plot(kind='bar', ax=axes[0, 1], color='lightcoral')\n",
    "axes[0, 1].set_title('Anomaly Rate by APOE4 Copies')\n",
    "axes[0, 1].set_ylabel('Anomaly Rate')\n",
    "axes[0, 1].set_xlabel('APOE4 Copies')\n",
    "\n",
    "# Age distribution\n",
    "normal_ages = patient_results[~patient_results['iso_anomaly']]['age']\n",
    "anomaly_ages = patient_results[patient_results['iso_anomaly']]['age']\n",
    "axes[0, 2].hist([normal_ages, anomaly_ages], bins=15, alpha=0.7, \n",
    "               label=['Normal', 'Anomaly'], color=['blue', 'red'])\n",
    "axes[0, 2].set_title('Age Distribution')\n",
    "axes[0, 2].set_xlabel('Age')\n",
    "axes[0, 2].set_ylabel('Frequency')\n",
    "axes[0, 2].legend()\n",
    "\n",
    "# Education analysis\n",
    "normal_edu = patient_results[~patient_results['iso_anomaly']]['education']\n",
    "anomaly_edu = patient_results[patient_results['iso_anomaly']]['education']\n",
    "axes[1, 0].boxplot([normal_edu, anomaly_edu], labels=['Normal', 'Anomaly'])\n",
    "axes[1, 0].set_title('Education Level Distribution')\n",
    "axes[1, 0].set_ylabel('Years of Education')\n",
    "\n",
    "# Anomaly score distribution by pattern\n",
    "for pattern in patient_results['pattern_type'].unique():\n",
    "    pattern_data = patient_results[patient_results['pattern_type'] == pattern]\n",
    "    axes[1, 1].hist(pattern_data['combined_score'], alpha=0.7, \n",
    "                   label=pattern, bins=20)\n",
    "axes[1, 1].set_title('Anomaly Score Distribution by Pattern')\n",
    "axes[1, 1].set_xlabel('Combined Anomaly Score')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "# Confusion matrix style analysis\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# True anomalies are rapid_decline, plateau, and fluctuating patterns\n",
    "true_anomalies = patient_results['pattern_type'] != 'normal'\n",
    "detected_anomalies = patient_results['iso_anomaly']\n",
    "\n",
    "cm = confusion_matrix(true_anomalies, detected_anomalies)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1, 2])\n",
    "axes[1, 2].set_title('Detection Performance\\n(True vs Detected Anomalies)')\n",
    "axes[1, 2].set_xlabel('Detected Anomaly')\n",
    "axes[1, 2].set_ylabel('True Anomaly')\n",
    "axes[1, 2].set_xticklabels(['Normal', 'Anomaly'])\n",
    "axes[1, 2].set_yticklabels(['Normal', 'Anomaly'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f86608",
   "metadata": {},
   "source": [
    "## 7. Cross-Validation {#cross-validation}\n",
    "\n",
    "We validate the temporal consistency of anomaly detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8a958a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal cross-validation\n",
    "from anomaly_detection import CrossValidator\n",
    "\n",
    "cv = CrossValidator('../config/config.yaml')\n",
    "splits = cv.temporal_split(df, patient_ids)\n",
    "\n",
    "print(f\"Created {len(splits)} temporal validation splits:\")\n",
    "for i, split in enumerate(splits):\n",
    "    print(f\"  Split {i+1}: Test month {split['test_month']}, {len(split['train_patients'])} patients\")\n",
    "\n",
    "if len(splits) > 0:\n",
    "    # Run cross-validation\n",
    "    consistency_results, all_scores = cv.validate_anomaly_consistency(\n",
    "        detector, splits, feature_engine, df\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nCross-validation results:\")\n",
    "    if 'error' not in consistency_results:\n",
    "        print(f\"  Common patients: {consistency_results.get('common_patients', 'N/A')}\")\n",
    "        print(f\"  Score correlation (mean): {consistency_results.get('score_correlation_mean', 'N/A'):.3f}\")\n",
    "        print(f\"  Score correlation (std): {consistency_results.get('score_correlation_std', 'N/A'):.3f}\")\n",
    "    else:\n",
    "        print(f\"  Error: {consistency_results['error']}\")\n",
    "else:\n",
    "    print(\"No valid temporal splits found for cross-validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5cae06",
   "metadata": {},
   "source": [
    "## 8. Conclusions {#conclusions}\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "1. **Anomaly Detection Performance**: \n",
    "   - Isolation Forest successfully identified atypical cognitive decline patterns\n",
    "   - DBSCAN provided complementary density-based anomaly detection\n",
    "   - Consensus approach improved robustness of anomaly identification\n",
    "\n",
    "2. **Clinical Relevance**:\n",
    "   - Rapid decline patterns were most frequently flagged as anomalies\n",
    "   - Plateau patterns (cognitive stability) were also identified as atypical\n",
    "   - Fluctuating patterns showed intermediate anomaly scores\n",
    "\n",
    "3. **Risk Factors**:\n",
    "   - APOE4 carriers showed higher rates of anomalous patterns\n",
    "   - Age and education showed associations with anomaly detection\n",
    "\n",
    "### Clinical Implications\n",
    "\n",
    "- **Early Identification**: Anomaly detection can flag patients with unusual progression patterns for closer monitoring\n",
    "- **Personalized Care**: Different anomaly types may require different intervention strategies\n",
    "- **Research Applications**: Identified anomalies could guide investigation into protective or risk factors\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- Harmonization assumes the ADNI visit schedule; other cohorts may require remapping\n",
    "- Feature engineering choices impact anomaly detection sensitivity\n",
    "- Clinical validation would be needed for real-world application\n",
    "\n",
    "### Future Directions\n",
    "\n",
    "1. **Enhanced Features**: Incorporate neuroimaging, biomarker, and genetic data\n",
    "2. **Deep Learning**: Use recurrent neural networks for trajectory modeling\n",
    "3. **Clinical Validation**: Test on real longitudinal cohort data\n",
    "4. **Intervention Studies**: Investigate targeted interventions for different anomaly types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97359bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary statistics\n",
    "print(\"FINAL ANALYSIS SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total patients analyzed: {len(patient_ids)}\")\n",
    "print(f\"Isolation Forest anomalies: {np.sum(iso_predictions == -1)} ({np.mean(iso_predictions == -1):.1%})\")\n",
    "print(f\"DBSCAN noise points: {np.sum(dbscan_predictions)} ({np.mean(dbscan_predictions):.1%})\")\n",
    "print(f\"Consensus anomalies: {consensus['intersection_count']} ({consensus['intersection_count']/len(patient_ids):.1%})\")\n",
    "print(f\"Method agreement rate: {consensus['agreement_rate']:.3f}\")\n",
    "\n",
    "print(f\"\\nPattern-specific detection rates:\")\n",
    "for pattern in ['normal', 'rapid_decline', 'plateau', 'fluctuating']:\n",
    "    pattern_mask = patient_results['pattern_type'] == pattern\n",
    "    if np.sum(pattern_mask) > 0:\n",
    "        detection_rate = np.mean(patient_results[pattern_mask]['iso_anomaly'])\n",
    "        print(f\"  {pattern}: {detection_rate:.1%}\")\n",
    "\n",
    "print(f\"\\nTop 5 most anomalous patients:\")\n",
    "top_5 = patient_results.nlargest(5, 'combined_score')\n",
    "for _, row in top_5.iterrows():\n",
    "    print(f\"  Patient {int(row['patient_id'])}: {row['combined_score']:.3f} ({row['pattern_type']})\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
